"""
è®­ç»ƒå¼•æ“æ ¸å¿ƒæ¨¡å—
åŸºäºPEFT/LoRAçš„æ¨¡å‹å¾®è°ƒ
æ”¯æŒå¤šå¹³å°ï¼šCUDA (NVIDIA), MPS (Apple Silicon), CPU
"""

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    PeftModel
)
from datasets import load_dataset
import json
import os
import platform
from typing import Dict, Any, Optional, Callable
from dataclasses import dataclass
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def get_device_info() -> Dict[str, Any]:
    """
    è‡ªåŠ¨æ£€æµ‹å½“å‰ç³»ç»Ÿçš„è®¡ç®—è®¾å¤‡
    æ”¯æŒ CUDA (NVIDIA GPU), MPS (Apple Silicon), CPU
    
    Returns:
        dict: åŒ…å«è®¾å¤‡ç±»å‹ã€è®¾å¤‡åç§°ã€å¯ç”¨æ€§ç­‰ä¿¡æ¯
    """
    device_info = {
        "platform": platform.system(),
        "machine": platform.machine(),
        "device": "cpu",
        "device_name": "CPU",
        "available": True,
        "cuda_available": False,
        "mps_available": False,
        "device_count": 0,
    }
    
    # æ£€æµ‹CUDA (NVIDIA GPU)
    if torch.cuda.is_available():
        device_info.update({
            "device": "cuda",
            "device_name": torch.cuda.get_device_name(0),
            "cuda_available": True,
            "device_count": torch.cuda.device_count(),
            "cuda_version": torch.version.cuda,
        })
        logger.info(f"âœ… æ£€æµ‹åˆ° NVIDIA GPU: {device_info['device_name']}")
        logger.info(f"   GPUæ•°é‡: {device_info['device_count']}")
        logger.info(f"   CUDAç‰ˆæœ¬: {device_info['cuda_version']}")
    
    # æ£€æµ‹MPS (Apple Silicon)
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device_info.update({
            "device": "mps",
            "device_name": "Apple Silicon (MPS)",
            "mps_available": True,
        })
        logger.info(f"âœ… æ£€æµ‹åˆ° Apple Silicon GPU (MPS)")
        logger.info(f"   ç³»ç»Ÿ: {device_info['platform']} {device_info['machine']}")
    
    # å›é€€åˆ°CPU
    else:
        logger.warning("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒ")
        logger.warning("   è®­ç»ƒé€Ÿåº¦ä¼šè¾ƒæ…¢ï¼Œå»ºè®®ä½¿ç”¨GPU")
    
    return device_info


def get_optimal_device() -> str:
    """
    è·å–æœ€ä¼˜è®¡ç®—è®¾å¤‡
    ä¼˜å…ˆçº§: CUDA > MPS > CPU
    
    Returns:
        str: è®¾å¤‡å­—ç¬¦ä¸² ("cuda", "mps", "cpu")
    """
    device_info = get_device_info()
    return device_info["device"]


def get_torch_dtype(device: str) -> torch.dtype:
    """
    æ ¹æ®è®¾å¤‡ç±»å‹è·å–æœ€ä¼˜çš„æ•°æ®ç±»å‹
    
    Args:
        device: è®¾å¤‡ç±»å‹ ("cuda", "mps", "cpu")
    
    Returns:
        torch.dtype: æ¨èçš„æ•°æ®ç±»å‹
    """
    if device == "cuda":
        # CUDAæ”¯æŒfloat16
        return torch.float16
    elif device == "mps":
        # MPSåœ¨æŸäº›æƒ…å†µä¸‹float16å¯èƒ½æœ‰é—®é¢˜ï¼Œä½¿ç”¨float32æ›´ç¨³å®š
        # å¦‚æœPyTorchç‰ˆæœ¬æ”¯æŒï¼Œå¯ä»¥ä½¿ç”¨bfloat16
        if torch.backends.mps.is_built():
            return torch.float32  # MPSç›®å‰å¯¹float16æ”¯æŒä¸å®Œå…¨
        return torch.float32
    else:
        # CPUä½¿ç”¨float32
        return torch.float32


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    model_name_or_path: str
    dataset_path: str
    output_dir: str
    
    # LoRAé…ç½®
    lora_rank: int = 8
    lora_alpha: int = 16
    lora_dropout: float = 0.05
    lora_target_modules: list = None
    
    # è®­ç»ƒå‚æ•°
    num_train_epochs: int = 3
    per_device_train_batch_size: int = 4
    per_device_eval_batch_size: int = 4
    gradient_accumulation_steps: int = 4
    learning_rate: float = 3e-4
    max_length: int = 512
    
    # ä¼˜åŒ–å‚æ•°
    warmup_steps: int = 100
    save_steps: int = 500
    logging_steps: int = 10
    eval_steps: int = 500
    save_total_limit: int = 3
    
    # ç¡¬ä»¶é…ç½®ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
    device: str = None  # è‡ªåŠ¨æ£€æµ‹: "cuda", "mps", "cpu"
    fp16: bool = None   # è‡ªåŠ¨æ ¹æ®è®¾å¤‡è®¾ç½®
    bf16: bool = False
    gradient_checkpointing: bool = True
    
    def __post_init__(self):
        if self.lora_target_modules is None:
            self.lora_target_modules = ["q_proj", "k_proj", "v_proj"]
        
        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
        if self.device is None:
            self.device = get_optimal_device()
            logger.info(f"ğŸ”§ è‡ªåŠ¨é€‰æ‹©è®¡ç®—è®¾å¤‡: {self.device.upper()}")
        
        # æ ¹æ®è®¾å¤‡è‡ªåŠ¨é…ç½®ç²¾åº¦
        if self.fp16 is None:
            if self.device == "cuda":
                self.fp16 = True
                logger.info("   å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (FP16)")
            elif self.device == "mps":
                self.fp16 = False  # MPSæš‚ä¸æ”¯æŒFP16
                logger.info("   ä½¿ç”¨FP32ç²¾åº¦ (MPS)")
            else:
                self.fp16 = False
                logger.info("   ä½¿ç”¨FP32ç²¾åº¦ (CPU)")


class LoRATrainer:
    """LoRAè®­ç»ƒå™¨"""
    
    def __init__(self, config: TrainingConfig, progress_callback: Optional[Callable] = None):
        self.config = config
        self.progress_callback = progress_callback
        self.model = None
        self.tokenizer = None
        self.trainer = None
        
    def load_model_and_tokenizer(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆæ”¯æŒå¤šå¹³å°ï¼‰"""
        logger.info(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {self.config.model_name_or_path}")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_name_or_path,
            trust_remote_code=True,
            use_fast=False
        )
        
        # è®¾ç½®pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # æ ¹æ®è®¾å¤‡é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹å’ŒåŠ è½½ç­–ç•¥
        torch_dtype = get_torch_dtype(self.config.device)
        
        # è®¾å¤‡æ˜ å°„ç­–ç•¥
        if self.config.device == "cuda":
            device_map = "auto"  # CUDAæ”¯æŒè‡ªåŠ¨åˆ†é…
        elif self.config.device == "mps":
            device_map = None    # MPSéœ€è¦æ‰‹åŠ¨ç®¡ç†
        else:
            device_map = None    # CPU
        
        logger.info(f"   æ•°æ®ç±»å‹: {torch_dtype}")
        logger.info(f"   è®¾å¤‡æ˜ å°„: {device_map if device_map else 'manual'}")
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name_or_path,
            trust_remote_code=True,
            torch_dtype=torch_dtype,
            device_map=device_map,
            low_cpu_mem_usage=True
        )
        
        # å¯¹äºMPSå’ŒCPUï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
        if self.config.device in ["mps", "cpu"]:
            self.model = self.model.to(self.config.device)
            logger.info(f"   æ¨¡å‹å·²ç§»åŠ¨åˆ° {self.config.device.upper()}")
        
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        if self.config.gradient_checkpointing:
            self.model.gradient_checkpointing_enable()
            logger.info("   å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
        
        logger.info("âœ… æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        
    def prepare_peft_model(self):
        """å‡†å¤‡PEFTæ¨¡å‹"""
        logger.info("Preparing PEFT model with LoRA")
        
        # åˆ›å»ºLoRAé…ç½®
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=self.config.lora_rank,
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            target_modules=self.config.lora_target_modules,
            bias="none",
            inference_mode=False
        )
        
        # åº”ç”¨LoRA
        self.model = get_peft_model(self.model, peft_config)
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°
        self.model.print_trainable_parameters()
        
        logger.info("PEFT model prepared successfully")
        
    def load_and_prepare_dataset(self):
        """åŠ è½½å’Œå‡†å¤‡æ•°æ®é›†"""
        logger.info(f"Loading dataset: {self.config.dataset_path}")
        
        # æ ¹æ®æ–‡ä»¶æ‰©å±•ååŠ è½½æ•°æ®é›†
        file_ext = self.config.dataset_path.split('.')[-1]
        
        if file_ext == 'json':
            dataset = load_dataset('json', data_files=self.config.dataset_path)
        elif file_ext == 'jsonl':
            dataset = load_dataset('json', data_files=self.config.dataset_path)
        elif file_ext == 'csv':
            dataset = load_dataset('csv', data_files=self.config.dataset_path)
        else:
            raise ValueError(f"Unsupported file format: {file_ext}")
        
        # æ•°æ®é¢„å¤„ç†å‡½æ•°
        def preprocess_function(examples):
            # å‡è®¾æ•°æ®æ ¼å¼åŒ…å« 'instruction', 'input', 'output' å­—æ®µ
            texts = []
            for i in range(len(examples.get('instruction', examples.get('text', [])))):
                if 'instruction' in examples:
                    instruction = examples['instruction'][i]
                    input_text = examples.get('input', [''] * len(examples['instruction']))[i]
                    output_text = examples['output'][i]
                    
                    # æ„å»ºè®­ç»ƒæ–‡æœ¬
                    if input_text:
                        text = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output_text}"
                    else:
                        text = f"### Instruction:\n{instruction}\n\n### Response:\n{output_text}"
                else:
                    text = examples['text'][i]
                
                texts.append(text)
            
            # åˆ†è¯
            tokenized = self.tokenizer(
                texts,
                truncation=True,
                max_length=self.config.max_length,
                padding="max_length",
                return_tensors="pt"
            )
            
            tokenized["labels"] = tokenized["input_ids"].clone()
            return tokenized
        
        # åº”ç”¨é¢„å¤„ç†
        processed_dataset = dataset.map(
            preprocess_function,
            batched=True,
            remove_columns=dataset["train"].column_names
        )
        
        # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
        if "validation" not in processed_dataset:
            split_dataset = processed_dataset["train"].train_test_split(test_size=0.1, seed=42)
            train_dataset = split_dataset["train"]
            eval_dataset = split_dataset["test"]
        else:
            train_dataset = processed_dataset["train"]
            eval_dataset = processed_dataset["validation"]
        
        logger.info(f"Dataset loaded: {len(train_dataset)} train, {len(eval_dataset)} eval")
        
        return train_dataset, eval_dataset
        
    def create_trainer(self, train_dataset, eval_dataset):
        """åˆ›å»ºè®­ç»ƒå™¨ï¼ˆæ”¯æŒå¤šå¹³å°ï¼‰"""
        logger.info("ğŸ”¨ åˆ›å»ºè®­ç»ƒå™¨")
        
        # æ ¹æ®è®¾å¤‡è°ƒæ•´è®­ç»ƒå‚æ•°
        use_mps_device = self.config.device == "mps"
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.config.output_dir,
            num_train_epochs=self.config.num_train_epochs,
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            per_device_eval_batch_size=self.config.per_device_eval_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            fp16=self.config.fp16 and self.config.device == "cuda",  # ä»…CUDAæ”¯æŒFP16
            bf16=self.config.bf16,
            warmup_steps=self.config.warmup_steps,
            save_steps=self.config.save_steps,
            logging_steps=self.config.logging_steps,
            eval_steps=self.config.eval_steps,
            evaluation_strategy="steps",
            save_strategy="steps",
            save_total_limit=self.config.save_total_limit,
            load_best_model_at_end=True,
            report_to=["tensorboard"],
            logging_dir=f"{self.config.output_dir}/logs",
            remove_unused_columns=False,
            use_mps_device=use_mps_device,  # å¯ç”¨MPSæ”¯æŒ
        )
        
        logger.info(f"   è®­ç»ƒè®¾å¤‡: {self.config.device.upper()}")
        logger.info(f"   æ··åˆç²¾åº¦: {'FP16' if training_args.fp16 else 'FP32'}")
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False
        )
        
        # åˆ›å»ºTrainer
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
        )
        
        logger.info("Trainer created successfully")
        
    def train(self):
        """æ‰§è¡Œè®­ç»ƒ"""
        logger.info("Starting training...")
        
        try:
            # è®­ç»ƒ
            train_result = self.trainer.train()
            
            # ä¿å­˜æ¨¡å‹
            self.trainer.save_model()
            
            # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
            metrics = train_result.metrics
            self.trainer.log_metrics("train", metrics)
            self.trainer.save_metrics("train", metrics)
            
            # è¯„ä¼°
            eval_metrics = self.trainer.evaluate()
            self.trainer.log_metrics("eval", eval_metrics)
            self.trainer.save_metrics("eval", eval_metrics)
            
            logger.info("Training completed successfully")
            
            return {
                "success": True,
                "train_metrics": metrics,
                "eval_metrics": eval_metrics
            }
            
        except Exception as e:
            logger.error(f"Training failed: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def save_lora_adapter(self, output_path: str):
        """ä¿å­˜LoRAé€‚é…å™¨"""
        logger.info(f"Saving LoRA adapter to {output_path}")
        self.model.save_pretrained(output_path)
        self.tokenizer.save_pretrained(output_path)
        logger.info("LoRA adapter saved successfully")
    
    def merge_and_save(self, output_path: str):
        """åˆå¹¶LoRAæƒé‡å¹¶ä¿å­˜å®Œæ•´æ¨¡å‹"""
        logger.info(f"Merging LoRA weights and saving to {output_path}")
        
        # åˆå¹¶æƒé‡
        merged_model = self.model.merge_and_unload()
        
        # ä¿å­˜
        merged_model.save_pretrained(output_path)
        self.tokenizer.save_pretrained(output_path)
        
        logger.info("Merged model saved successfully")
        
    def run_full_training(self):
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        try:
            # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
            self.load_model_and_tokenizer()
            
            # 2. å‡†å¤‡PEFTæ¨¡å‹
            self.prepare_peft_model()
            
            # 3. åŠ è½½æ•°æ®é›†
            train_dataset, eval_dataset = self.load_and_prepare_dataset()
            
            # 4. åˆ›å»ºè®­ç»ƒå™¨
            self.create_trainer(train_dataset, eval_dataset)
            
            # 5. è®­ç»ƒ
            result = self.train()
            
            # 6. ä¿å­˜LoRAé€‚é…å™¨
            lora_adapter_path = os.path.join(self.config.output_dir, "lora_adapter")
            self.save_lora_adapter(lora_adapter_path)
            
            # 7. å¯é€‰ï¼šä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹
            # merged_model_path = os.path.join(self.config.output_dir, "merged_model")
            # self.merge_and_save(merged_model_path)
            
            return result
            
        except Exception as e:
            logger.error(f"Training pipeline failed: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }


def calculate_catastrophic_forgetting_score(
    base_model_path: str,
    finetuned_model_path: str,
    eval_dataset_path: str
) -> float:
    """
    è®¡ç®—ç¾éš¾æ€§é—å¿˜åˆ†æ•° (CF Score)
    é€šè¿‡æ¯”è¾ƒåŸºåº§æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹åœ¨é€šç”¨æ•°æ®é›†ä¸Šçš„æ€§èƒ½
    """
    # TODO: å®ç°CF Scoreè®¡ç®—é€»è¾‘
    # 1. åŠ è½½åŸºåº§æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹
    # 2. åœ¨é€šç”¨è¯„ä¼°é›†ä¸Šè®¡ç®—äº¤å‰ç†µæŸå¤±
    # 3. è¿”å›æŸå¤±å·®å¼‚
    pass


if __name__ == "__main__":
    import argparse
    import sys
    
    parser = argparse.ArgumentParser(description='LoRA Training Script')
    parser.add_argument('--config', type=str, help='Path to training config JSON file')
    parser.add_argument('--task-id', type=int, help='Task ID for tracking')
    args = parser.parse_args()
    
    if args.config:
        # ä»é…ç½®æ–‡ä»¶åŠ è½½
        logger.info(f"Loading config from {args.config}")
        with open(args.config, 'r', encoding='utf-8') as f:
            config_dict = json.load(f)
        
        # åˆ›å»ºè®­ç»ƒé…ç½®
        config = TrainingConfig(
            model_name_or_path=config_dict.get('model_path', 'meta-llama/Llama-2-7b-hf'),
            dataset_path=config_dict.get('dataset_path', './data/train.json'),
            output_dir=config_dict.get('output_dir', './outputs/lora_model'),
            lora_rank=config_dict.get('lora_rank', 8),
            lora_alpha=config_dict.get('lora_alpha', 16),
            learning_rate=config_dict.get('learning_rate', 1e-4),
            num_train_epochs=config_dict.get('num_epochs', 3),
            per_device_train_batch_size=config_dict.get('batch_size', 4),
            gradient_accumulation_steps=config_dict.get('gradient_accumulation_steps', 4),
            warmup_steps=config_dict.get('warmup_steps', 100),
            logging_steps=config_dict.get('logging_steps', 10),
            save_steps=config_dict.get('save_steps', 500),
            lora_target_modules=config_dict.get('lora_target_modules', ["q_proj", "v_proj"]),
        )
        
        logger.info(f"Task ID: {args.task_id}")
        logger.info(f"Config loaded: model={config.model_name_or_path}, dataset={config.dataset_path}")
        
        # è¿è¡Œè®­ç»ƒ
        trainer = LoRATrainer(config)
        result = trainer.run_full_training()
        
        # è¾“å‡ºç»“æœ
        logger.info(f"Training result: {result}")
        print(json.dumps(result, ensure_ascii=False))
        
        # è¿”å›é€€å‡ºç 
        sys.exit(0 if result.get('success', False) else 1)
    else:
        # æµ‹è¯•æ¨¡å¼ï¼ˆæ— é…ç½®æ–‡ä»¶ï¼‰
        logger.warning("No config file provided, running in test mode")
        config = TrainingConfig(
            model_name_or_path="meta-llama/Llama-2-7b-hf",
            dataset_path="./data/train.json",
            output_dir="./outputs/test_run",
            lora_rank=8,
            lora_alpha=16,
            num_train_epochs=3,
        )
        
        trainer = LoRATrainer(config)
        result = trainer.run_full_training()
        print(json.dumps(result, ensure_ascii=False))

